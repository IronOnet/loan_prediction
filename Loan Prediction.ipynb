{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14509c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102d582",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths to where the data is stored\n",
    "train_data_path = './data/d_training_set.csv' # change this to /path/to/file \n",
    "test_data_path = './data/d_test_set.csv' # change this to /path/to/file\n",
    "\n",
    "train_data = pd.read_csv(train_data_path) \n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16c5e0",
   "metadata": {},
   "source": [
    "### Preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81885884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to ease the preprocessing \n",
    "# steps, this will remove nan values \n",
    "# and returns the preprocessed data with \n",
    "# the lists of categorical and numerical \n",
    "# columns in the data and the row id \n",
    "# if the data to be preprocessed is the test set\n",
    "def process_data(dframe, test=None):\n",
    "    df = dframe\n",
    "    print('Data Processing Started')\n",
    "    if not test: \n",
    "        # drop the row_id column on the column axis\n",
    "        df= df.drop('row_id', axis=1)\n",
    "        cat_features = (df.dtypes == 'object') \n",
    "        cat_features = list(cat_features[cat_features].index) \n",
    "        num_features = (df.dtypes != 'object') \n",
    "        num_features = list(num_features[num_features].index) \n",
    "\n",
    "        # the slice of the data frame with categorical \n",
    "        # features\n",
    "        df_cat = df[cat_features] \n",
    "        # the slice of the data frame with numerical features\n",
    "        df_num  = df[num_features] \n",
    "\n",
    "        # fill empty categories row with 'Unknown'\n",
    "        df_cat.fillna('Unknown', axis=1, inplace=True)\n",
    "        # Additional numerical features in the dataset \n",
    "        num_features_2 = ['term', 'int_rate', 'emp_length', 'issue_d', 'revol_util'] \n",
    "\n",
    "        # Replacing the percentage symbol with an empty string \n",
    "        df_cat['int_rate'] = df_cat['int_rate'].replace({'%': ''}, regex=True) \n",
    "        # convert the string to a float \n",
    "        df_cat['int_rate'] = df_cat['int_rate'].astype(float) \n",
    "        # Replacing string symbols with integers \n",
    "        df_cat['term'] = df_cat['term'].replace({' 36 months': 36, ' 60 months': 60}, regex=True)\n",
    "        df_cat['emp_length'] = df_cat['emp_length'].replace({'years': '', '10+': 10, 'year': '', '< 1 year': 1}, regex=True)\n",
    "        # drop the 'issue_d' column as we deem it unnecessary for the model \n",
    "        df_cat.drop('issue_d', axis=1, inplace=True)\n",
    "        df_cat['revol_util'] = df_cat['revol_util'].replace({'%': ''}, regex=True) \n",
    "        df_cat['revol_util'] = df_cat['revol_util'].replace({'Unknown': 0}, regex=True)\n",
    "        df_cat['revol_util'] = df_cat['revol_util'].astype(float) \n",
    "        df_cat['term'] = df_cat['term'].astype(float)\n",
    "        num_features_2.remove('issue_d')\n",
    "\n",
    "        # numerical features extracted from the slice \n",
    "        # with categorical features\n",
    "        df_num_2 = df_cat[num_features_2] \n",
    "        df_num = pd.concat([df_num, df_num_2], axis=1)\n",
    "        # Filling missing variables with the mean of each column \n",
    "        df_num.fillna(df_num.mean(), inplace=True) \n",
    "        # drop the new numeric features from the categorical dataframe\n",
    "        df_cat = df_cat.drop(num_features_2, axis=1)\n",
    "\n",
    "        df_num['emp_length'] = df_num['emp_length'].replace({'< 1': 1}, regex=True) \n",
    "        df_num = df_num.replace({'Unknown': 0}, regex=True)\n",
    "\n",
    "        df_combined = pd.concat([df_num, df_cat], axis=1)\n",
    "        # categorical features (or attributes) \n",
    "        cat_attribs = list(df_cat.columns) \n",
    "        num_attribs = list(df_num.columns) \n",
    "        if 'repaid_loan' in num_attribs:\n",
    "            num_attribs.remove('repaid_loan')\n",
    "        print('*************Training data processed!!*************')\n",
    "        return df_combined, cat_attribs, num_attribs\n",
    "\n",
    "    elif test: \n",
    "        # if it's the test set \n",
    "        if 'row_id' in list(df.columns):\n",
    "            \n",
    "            row_id_col = df[['row_id']].copy() \n",
    "            df=df.drop('row_id', axis=1)\n",
    "        cat_features = (df.dtypes == 'object') \n",
    "        cat_features = list(cat_features[cat_features].index) \n",
    "        num_features = (df.dtypes != 'object') \n",
    "        num_features = list(num_features[num_features].index) \n",
    "\n",
    "        df_cat = df[cat_features] \n",
    "        df_num  = df[num_features] \n",
    "\n",
    "        df_cat.fillna('Unknown', axis=1, inplace=True)\n",
    "        # Additional numerical features in the dataset \n",
    "        num_features_2 = ['term', 'int_rate', 'emp_length', 'issue_d', 'revol_util'] \n",
    "\n",
    "        # Replacing the percentage symbol with an empty string \n",
    "        df_cat['int_rate'] = df_cat['int_rate'].replace({'%': ''}, regex=True) \n",
    "        # convert the string to a float \n",
    "        df_cat['int_rate'] = df_cat['int_rate'].astype(float) \n",
    "        # Replacing string symbols with integers \n",
    "        df_cat['term'] = df_cat['term'].replace({' 36 months': 36, ' 60 months': 60}, regex=True)\n",
    "        df_cat['emp_length'] = df_cat['emp_length'].replace({'years': '', '10+': 10, 'year': '', '< 1 year': 1}, regex=True)\n",
    "        # drop the 'issue_d' column as we deem it unnecessary for the model \n",
    "        df_cat.drop('issue_d', axis=1, inplace=True)\n",
    "        df_cat['revol_util'] = df_cat['revol_util'].replace({'%': ''}, regex=True) \n",
    "        df_cat['revol_util'] = df_cat['revol_util'].replace({'Unknown': 0}, regex=True)\n",
    "        df_cat['revol_util'] = df_cat['revol_util'].astype(float) \n",
    "        df_cat['term'] = df_cat['term'].astype(float)\n",
    "        num_features_2.remove('issue_d')\n",
    "\n",
    "        df_num_2 = df_cat[num_features_2] \n",
    "        df_num = pd.concat([df_num, df_num_2], axis=1)\n",
    "        # Filling missing variables with the mean of each column \n",
    "        df_num.fillna(df_num.mean(), inplace=True) \n",
    "        # drop the new numeric features from the categorical dataframe\n",
    "        df_cat = df_cat.drop(num_features_2, axis=1)\n",
    "\n",
    "        df_num['emp_length'] = df_num['emp_length'].replace({'< 1': 1}, regex=True) \n",
    "        df_num = df_num.replace({'Unknown': 0}, regex=True)\n",
    "\n",
    "        df_combined = pd.concat([df_num, df_cat], axis=1)\n",
    "        # categorical features (or attributes) \n",
    "        cat_attribs = list(df_cat.columns) \n",
    "        num_attribs = list(df_num.columns) \n",
    "        print('*************Test data processed!!***************')\n",
    "        return df_combined, cat_attribs, num_attribs,   row_id_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the processed version of the train_set\n",
    "X_train_proc, cat_attribs, num_attribs = process_data(train_data)\n",
    "\n",
    "\n",
    "# The slice of the dataset containing numeric features, plus the labels\n",
    "X_train_num, y_train = X_train_proc[num_attribs], X_train_proc['repaid_loan']\n",
    "\n",
    "# The processed version of the test_set\n",
    "X_test_proc, _, _, row_id_col = process_data(test_data, test=True)\n",
    "# The slice of the test set containing numeric features\n",
    "X_test_num = X_test_proc[num_attribs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01658270",
   "metadata": {},
   "source": [
    "### Pipeline to further preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num' , num_pipeline, num_attribs), \n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformed version of both training and test dataset\n",
    "train_data_prepared = full_pipeline.fit_transform(X_train_num)\n",
    "test_data_prepared = full_pipeline.fit_transform(X_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c213a73",
   "metadata": {},
   "source": [
    "### Train a Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to train a LinearRegression Model to see the results\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "lin_reg = LinearRegression() \n",
    "lin_reg.fit(train_data_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well the model performs on a sample \n",
    "# of the data\n",
    "some_data = X_train_num.iloc[:5] \n",
    "some_labels = y_train.iloc[:5] \n",
    "some_data_prepared = full_pipeline.transform(some_data) \n",
    "print(\"Predictions: \", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8571d63",
   "metadata": {},
   "source": [
    "### Model Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f209b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_squared_error \n",
    "# to compute the error rate of the model\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "loan_predictions = lin_reg.predict(train_data_prepared) \n",
    "lin_mse = mean_squared_error(y_train, loan_predictions) \n",
    "lin_rmse = np.sqrt(lin_mse) \n",
    "print('LR min_squared_error : {:.3f}'.format(lin_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f029eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "lin_mae = mean_absolute_error(y_train, loan_predictions) \n",
    "print('LR mean squared error {}'.format(lin_mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b052cc6",
   "metadata": {},
   "source": [
    "### Training A Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e596961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a DecisionTree\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(train_data_prepared, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2af18",
   "metadata": {},
   "source": [
    "####  Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5555cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on the training set\n",
    "loan_predictions = tree_reg.predict(train_data_prepared) \n",
    "tree_mse = mean_squared_error(y_train, loan_predictions) \n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "scores = cross_val_score(tree_reg, train_data_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10) \n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: \", scores) \n",
    "    print(\"Mean: \", scores.mean()) \n",
    "    print(\"STD: \", scores.std()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7245f2",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab448f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMporting a random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "# Initilializing a RandomFOrestRegressor with 10 estimators \n",
    "# The random_state ensures that weights are initaliazed consistently\n",
    "forest_reg = RandomForestRegressor(n_estimators=10, random_state=42) \n",
    "forest_reg.fit(train_data_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of the forest regressor on the training set\n",
    "forest_reg_predictions = forest_reg.predict(train_data_prepared) \n",
    "forest_mse = mean_squared_error(y_train, forest_reg_predictions) \n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddee470",
   "metadata": {},
   "source": [
    "#### Model Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd256c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation score, \n",
    "# The model will be evaluated on 10 folds of \n",
    "# the shuffled training data\n",
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "forest_scores = cross_val_score(forest_reg, train_data_prepared, y_train, \n",
    "                               scoring=\"neg_mean_squared_error\", cv=10) \n",
    "forest_rmse_scores = np.sqrt(-forest_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f164ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa146574",
   "metadata": {},
   "source": [
    "### GridSearch cross validation to find the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7050445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search to find the best performing model\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# Hyper parameter search space\n",
    "param_grid = [\n",
    "    # try 12 (3x4) combinations of hyperparameters \n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, \n",
    "    {'bootstrap' : [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42) \n",
    "# train accross 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, \n",
    "                          scoring='neg_mean_squared_error', return_train_score=True) \n",
    "grid_search.fit(train_data_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO]: \\n')\n",
    "print('********Best Parameters for GridSearch Cross Validation: {}'.format(grid_search.best_params_))\n",
    "\n",
    "print('********Best Estimator for GridSearch Cross validation: {}'.format(grid_search.best_estimator_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de347ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results \n",
    "print('********[CROSS VALIDATION RESULTS]*********')\n",
    "cvres = grid_search.cv_results_ \n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]): \n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddf982",
   "metadata": {},
   "source": [
    "#### Randomized Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from scipy.stats import randint \n",
    "\n",
    "param_distribs = {\n",
    "    'n_estimators': randint(low=1, high=200), \n",
    "    'max_features': randint(low=1, high=8),\n",
    "}\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42) \n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, \n",
    "                               n_iter=10, cv=5, scoring=\"neg_mean_squared_error\", random_state=42) \n",
    "rnd_search.fit(train_data_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319baa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('***********[RANDOMIZED SEARCH CROSS VALIDATION RESULTS]************')\n",
    "cv_res = rnd_search.cv_results_ \n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]): \n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a73e2",
   "metadata": {},
   "source": [
    "#### Selecting the Best model and Saving its Predictions to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking the best performing model to make predictions\n",
    "final_tree_model = grid_search.best_estimator_ \n",
    "final_test_predictions = final_tree_model.predict(X_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO]: ************ PREDICTONS ON THE TEST SET **************')\n",
    "print('Final test_set predictions: {}'.format(final_test_predictions))\n",
    "\n",
    "\n",
    "# Concatenate row_id and repaid_loan columns \n",
    "# save it to a csv file called loan_predictions.csv\n",
    "predictions_pd = pd.DataFrame(final_test_predictions, columns=['repaid_loan'])\n",
    "predictions_csv = pd.concat([row_id_col, predictions_pd], axis=1)\n",
    "predictions_csv.to_csv('loan_predictions.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bea51",
   "metadata": {},
   "source": [
    "### Exploring Other Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the future if time is given we can explore neural networks \n",
    "# and other complex architecture to achieve the best results\n",
    "from sklearn.neural_network import MLPRegressor \n",
    "\n",
    "mlp = MLPRegressor(solver='sgd', max_iter=100, activation='relu', \n",
    "                  random_state=42, learning_rate_init=0.01, \n",
    "                  batch_size=train_data_prepared.shape[0], momentum= 0.04)\n",
    "print('[INFO] ****** Training a Multilayer Perceptron A.K.A Neural Network ******')\n",
    "mlp.fit(train_data_prepared, y_train)\n",
    "\n",
    "\n",
    "\n",
    "mlp_scores = cross_val_score(mlp, train_data_prepared, y_train, \n",
    "                               scoring=\"neg_mean_squared_error\", cv=10) \n",
    "mlp_rmse_scores = np.sqrt(-forest_scores) \n",
    "print('[INFO] ********* MLP SCORE *********')\n",
    "display_scores(mlp_rmse_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80abc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try ensemble methods\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.ensemble import AdaBoostRegressor \n",
    "\n",
    "num_trees = 30 \n",
    "kfold = KFold(n_splits=10) \n",
    "model = AdaBoostRegressor(n_estimators=num_trees, random_state=42) \n",
    "results = cross_val_score(model, X_train_num, y_train, cv=kfold) \n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
